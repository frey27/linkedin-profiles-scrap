{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages for the project \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "print('- Finish importing packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Login to Linkedin\n",
    "\n",
    "# Task 1.1: Open Chrome and Access Linkedin login site\n",
    "browser = webdriver.Chrome()\n",
    "browser.get('https://www.linkedin.com/uas/login')\n",
    "file = open('credentials.txt')\n",
    "lines = file.readlines()\n",
    "username = lines[0]\n",
    "password = lines[1]\n",
    "elementID = browser.find_element_by_id('username')\n",
    "elementID.send_keys(username)\n",
    "\n",
    "elementID = browser.find_element_by_id('password')\n",
    "elementID.send_keys(password)\n",
    "\n",
    "elementID.submit()\n",
    "print('- Finish Task 1: Login to Linkedin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Search for the profile we want to crawl\n",
    "\n",
    "# Task 2.1: Locate the search bar element\n",
    "search_field = browser.find_element_by_xpath('//*[@id=\"global-nav-typeahead\"]/input')\n",
    "\n",
    "# Task 2.2: Input the search query to the search bar\n",
    "search_query = input('What profile do you want to scrape? ')\n",
    "search_field.send_keys(search_query)\n",
    "\n",
    "# Task 2.3: Search\n",
    "search_field.send_keys(Keys.RETURN)\n",
    "\n",
    "print('- Finish Task 2: Search for profiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Task 3: Scrape the URLs of the profiles\n",
    "\n",
    "# Task 3.1: Write a function to extract the URLs of one page\n",
    "def GetURL():\n",
    "    page_source = BeautifulSoup(browser.page_source)\n",
    "    profiles = page_source.find_all('a', class_ = 'app-aware-link')\n",
    "    all_profile_URL = []\n",
    "    for profile in profiles:\n",
    "        profile_ID = profile.get('href')\n",
    "        profile_URL = profile_ID\n",
    "        if profile_URL not in all_profile_URL:\n",
    "            if profile_URL.find(\"/in/\") > 0: #Quitar URLs que no necesitamos\n",
    "                all_profile_URL.append(profile_URL)     \n",
    "    return all_profile_URL\n",
    "\n",
    "# Task 3.2: Navigate through many page, and extract the profile URLs of each page\n",
    "\n",
    "input_page = int(input('How many pages you want to scrape: '))\n",
    "URLs_all_page = []\n",
    "for page in range(input_page):\n",
    "    URLs_one_page = GetURL()\n",
    "    sleep(2)\n",
    "    browser.execute_script('window.scrollTo(0, document.body.scrollHeight);') #scroll to the end of the page\n",
    "    sleep(3)\n",
    "    next_button = browser.find_element_by_class_name(\"artdeco-pagination__button--next\")\n",
    "    browser.execute_script(\"arguments[0].click();\", next_button)\n",
    "    URLs_all_page = URLs_all_page + URLs_one_page\n",
    "    sleep(2)\n",
    "\n",
    "print('- Finish Task 3: Scrape the URLs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Scrape the data of 1 Linkedin profile, and write the data to a .CSV file\n",
    "headers = ['Name', 'Location', 'Job Title', 'URL']\n",
    "info = []\n",
    "link_excel = \"borrar esta l√≠nea\"\n",
    "\n",
    "for linkedin_URL in URLs_all_page:\n",
    "    page_source = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    soup = page_source\n",
    "    try:\n",
    "        sleep(2)\n",
    "        browser.get(linkedin_URL)\n",
    "        sleep(1)\n",
    "        print('- Accessing profile: ', linkedin_URL)\n",
    "        print(link_excel)\n",
    "        link = link_excel\n",
    "        info_div = page_source.find('div',{'class':'flex-1 mr5'})\n",
    "        location_tag = soup.find('div', {'class': 'pb2 pv-text-details__left-panel'})\n",
    "        info_loc = location_tag.find_all(\"span\", {'class': 'text-body-small'})\n",
    "        name_div = soup.find('div', {'class': 'pv-text-details__left-panel'})\n",
    "        name =  name_div.find('h1').get_text().strip() #Remove unnecessary characters \n",
    "        print('--- Profile name is: ', name)\n",
    "        location = info_loc[0].get_text().strip() #Remove unnecessary characters \n",
    "        print('--- Profile location is: ', location)\n",
    "        info_title = soup.find('div', {'class': 'text-body-medium break-words'})\n",
    "        title = info_title.get_text().strip()\n",
    "        print('--- Profile title is: ', title)\n",
    "        link_excel = browser.current_url\n",
    "    except:\n",
    "        print(\"Error al acceder\")\n",
    "        continue    \n",
    "    print(link_excel)\n",
    "    info.append([name, location, title, link])\n",
    "    \n",
    "    #writer.writerow({headers[0]:name, headers[1]:location, headers[2]:title, headers[3]:linkedin_URL})\n",
    "    print('\\n')\n",
    "df = pd.DataFrame(info, columns=headers)\n",
    "df.to_excel('data.xlsx', index=False)\n",
    "print('Mission Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
